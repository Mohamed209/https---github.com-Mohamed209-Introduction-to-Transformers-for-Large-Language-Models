{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the architecture from (Attention Is All You Need) https://arxiv.org/abs/1706.03762\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic architecture \n",
    "\n",
    "![image.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12cdf506-6cd8-4afa-93a3-b77b82770309_2755x1570.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.gif](https://i.imgur.com/KgZCdzX.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical way to implement the values of the embedding is by hard coding them by using a sine and cosine function of the vectors and elementsâ€™ positions\n",
    "\n",
    "![image.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a77f49-ed6d-4614-9c64-505455bd0c83_2043x1300.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_size: int, d_attn: int):\n",
    "        \"\"\"represent positional encoding as harcoded matrix of size (context_size,d_attn)\n",
    "\n",
    "        Args:\n",
    "            context_size (int): max context size\n",
    "            d_attn (int): model hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(\n",
    "            size=(context_size, d_attn)\n",
    "        )  # placeholder matrix of the encoding , check above figures (orange matrix)\n",
    "        pos = torch.arange(0, context_size).unsqueeze(\n",
    "            dim=1\n",
    "        )  # positions are ranged from 0 to context size (those are rows indexes in orange matrix in above figures)\n",
    "        i = torch.arange(\n",
    "            0, d_attn, 2\n",
    "        )  # i range from 0 to d_attn in every pos (row in orange matrix)\n",
    "        arg = pos / (10000 ** (2 * i / d_attn))\n",
    "        self.encoding[:, 0::2] = torch.sin(arg)  # even columns (even i)\n",
    "        self.encoding[:, 1::2] = torch.cos(arg)  # odd i\n",
    "\n",
    "    def forward(self, tokens_sequence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"encode embedded tokens sequence\n",
    "\n",
    "        Args:\n",
    "            tokens_sequence (torch.Tensor):\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: position encoded embedded tokens\n",
    "        \"\"\"\n",
    "        return self.encoding[\n",
    "            : tokens_sequence.shape[1], :\n",
    "        ]  # just query the self.encoding matrix with tokens sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder block is composed of a multi-head attention layer, a position-wise feed-forward network, and two-layer normalization.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6627a678-0582-4950-a829-a8e9e4e97db9_3289x1326.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention layer allows to learn complex relationships between the hidden states, whereas the position-wise feed-forward network allows to learn complex relationships between the different elements within each vector.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf4bdbd2-8e45-4f33-9c86-35eede3571ab_3433x1050.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_attn, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_attn, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_attn)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_attention_heads: int, d_ff: int, d_attn: int):\n",
    "        \"\"\"init encoder\n",
    "\n",
    "        Args:\n",
    "            n_attention_heads (int): number of attention heads\n",
    "            d_ff (int): dimention feed forward network\n",
    "            d_attn (int): encoder hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_attn, n_attention_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_attn, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_attn)\n",
    "        self.norm2 = nn.LayerNorm(d_attn)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): hidden state tensor is elementwise addition between token embeddings and positional embeddings for the input sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: encoder projection tensor (encoder output)\n",
    "        \"\"\"\n",
    "        out1 = (\n",
    "            self.self_attn(query=hidden_states, key=hidden_states, value=hidden_states)[\n",
    "                0\n",
    "            ]\n",
    "            + hidden_states  # apply resiudal connection\n",
    "        )  # perform self attention on hidden states (note hidden state tensor is elementwise addition between token embeddings and positional embeddings)\n",
    "        norm1 = self.norm1(out1)  # layer normalization\n",
    "        out2 = self.feed_forward(norm1) + norm1\n",
    "        out3 = self.norm2(out2)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is just the token embedding and the position embedding followed by multiple encoder blocks.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3808c9f-715e-4ab0-be11-34e16b3d8644_3540x1022.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        context_size: int,\n",
    "        n_blocks: int,\n",
    "        n_heads: int,\n",
    "        d_attn: int,\n",
    "        d_ff: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_attn)\n",
    "        self.pos_embedding = PositionalEncoding(context_size, d_attn)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    d_attn=d_attn,\n",
    "                    n_attention_heads=n_heads,\n",
    "                    d_ff=d_ff,\n",
    "                )\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens_seq: torch.Tensor) -> torch.Tensor:\n",
    "        embedded_tokens = self.embedding(\n",
    "            tokens_seq\n",
    "        )  # apply embeddings layer to tokens input sequence\n",
    "        pos_embedded_tokens = self.pos_embedding(tokens_seq).cuda()\n",
    "        hidden_states = embedded_tokens + pos_embedded_tokens\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Block\n",
    "\n",
    "The decoder block is composed of a multi-head attention layer, a position-wise feed-forward network, a cross-attention layer, and three layer normalization.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0287aa3-7a69-41c4-a692-c1940e007f29_3301x1582.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_attn: int, num_heads: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_attn, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_attn)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_attn, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_attn)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_attn, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_attn)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, encoder_output: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): hidden state tensor is elementwise addition between token embeddings and positional embeddings for the output sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        out1 = (\n",
    "            self.self_attn(query=hidden_states, key=hidden_states, value=hidden_states)[\n",
    "                0\n",
    "            ]\n",
    "            + hidden_states\n",
    "        )  # apply resiudal connection\n",
    "        out1 = self.norm1(out1)\n",
    "        # apply corss attention between out1 and encoder output\n",
    "        out2 = (\n",
    "            self.cross_attn(query=out1, key=encoder_output, value=encoder_output)[0]\n",
    "            + out1\n",
    "        )\n",
    "        out2 = self.norm2(out2)\n",
    "        out3 = self.feed_forward(out2) + out2\n",
    "        out3 = self.norm3(out3)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cross-attention layer computes the attentions between the decoder's hidden states and the encoder output\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4f653-3985-40ac-932d-3eb023be2eb0_2723x1332.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is just the token embedding and the position embedding followed by multiple decoder blocks and the predicting head.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0808f5de-f713-4a56-8750-ac1cda39b929_2753x1542.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicting head is just a linear layer that projects the last hidden states from the d_attn dimension to the size of the vocabulary. To predict, we perform an ArgMax function on the resulting probability vectors\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc452d478-581f-4baf-941f-0ab07a39bdb3_3386x1342.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, context_size, d_attn, d_ff, num_heads, n_blocks):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, d_attn)\n",
    "        self.pos_embedding = PositionalEncoding(context_size, d_attn)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    d_attn=d_attn,\n",
    "                    num_heads=num_heads,\n",
    "                    d_ff=d_ff,\n",
    "                )\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.out = nn.Linear(d_attn, output_size)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        x = self.embedding(x) + self.pos_embedding(x).cuda()\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_output)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_vocab_size,\n",
    "        decoder_vocab_size,\n",
    "        context_size,\n",
    "        d_attn,\n",
    "        d_ff,\n",
    "        num_heads,\n",
    "        n_blocks,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            encoder_vocab_size, context_size, n_blocks, num_heads, d_attn, d_ff\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            decoder_vocab_size, context_size, d_attn, d_ff, num_heads, n_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, input_encoder, input_decoder):\n",
    "        enc_output = self.encoder(input_encoder)\n",
    "        output = self.decoder(input_decoder, enc_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the architecture with dummy data\n",
    "# SOS_token = 0\n",
    "# EOS_token = 1\n",
    "# PAD_token = 2\n",
    "# index2words = {SOS_token: \"SOS\", EOS_token: \"EOS\", PAD_token: \"PAD\"}\n",
    "# words = \"How are you doing ? I am good and you ?\"\n",
    "# words_list = set(words.lower().split(\" \"))\n",
    "# for word in words_list:\n",
    "#     index2words[len(index2words)] = word\n",
    "\n",
    "# words2index = {w: i for i, w in index2words.items()}\n",
    "\n",
    "\n",
    "# def convert2tensors(sentence, max_len):\n",
    "#     words_list = sentence.lower().split(\" \")\n",
    "#     padding = [\"PAD\"] * (max_len - len(words_list))\n",
    "#     words_list.extend(padding)\n",
    "#     indexes = [words2index[word] for word in words_list]\n",
    "#     return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "\n",
    "# d_attn = 10\n",
    "# VOCAB_SIZE = len(words2index)\n",
    "# N_BLOCKS = 10\n",
    "# D_FF = 20\n",
    "# CONTEXT_SIZE = 100\n",
    "# NUM_HEADS = 2\n",
    "# transformer = Transformer(\n",
    "#     encoder_vocab_size=VOCAB_SIZE,\n",
    "#     decoder_vocab_size=VOCAB_SIZE,\n",
    "#     context_size=CONTEXT_SIZE,\n",
    "#     d_attn=d_attn,\n",
    "#     d_ff=D_FF,\n",
    "#     num_heads=NUM_HEADS,\n",
    "#     n_blocks=N_BLOCKS,\n",
    "# )\n",
    "# input_sentence = \"How are you doing ?\"\n",
    "# output_sentence = \"I am good and\"\n",
    "# input_encoder = convert2tensors(input_sentence, CONTEXT_SIZE)\n",
    "# input_decoder = convert2tensors(output_sentence, CONTEXT_SIZE)\n",
    "# output = transformer(input_encoder, input_decoder)\n",
    "# _, indexes = output.squeeze().topk(1)\n",
    "# index2words[indexes[3].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the transformer architecture on machine translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = (\n",
    "            self.seq_len - len(enc_input_tokens) - 2\n",
    "        )  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * enc_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 1,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_blocks\": 8,\n",
    "        \"d_ff\": 1024,\n",
    "        \"datasource\": \"opus_books\",\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizers/tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\",\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[\"translation\"][lang]\n",
    "\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(\n",
    "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "        )\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(\n",
    "        f\"{config['datasource']}\",\n",
    "        f\"{config['lang_src']}-{config['lang_tgt']}\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(\n",
    "        train_ds_raw,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        config[\"lang_src\"],\n",
    "        config[\"lang_tgt\"],\n",
    "        config[\"seq_len\"],\n",
    "    )\n",
    "    val_ds = BilingualDataset(\n",
    "        val_ds_raw,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        config[\"lang_src\"],\n",
    "        config[\"lang_tgt\"],\n",
    "        config[\"seq_len\"],\n",
    "    )\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, batch_size=config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design\n",
      "Device memory: 5.7974853515625 GB\n",
      "Max length of source sentence: 316\n",
      "Max length of target sentence: 287\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(22439, 512)\n",
      "    (pos_embedding): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x EncoderBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(30000, 512)\n",
      "    (pos_embedding): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x DecoderBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (out): Linear(in_features=512, out_features=30000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "config = get_config()\n",
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "    print(\n",
    "        f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\"\n",
    "    )\n",
    "# Make sure the weights folder exists\n",
    "Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = Transformer(\n",
    "    encoder_vocab_size=tokenizer_src.get_vocab_size(),\n",
    "    decoder_vocab_size=tokenizer_tgt.get_vocab_size(),\n",
    "    context_size=config[\"seq_len\"],\n",
    "    d_attn=config[\"d_model\"],\n",
    "    num_heads=config[\"n_heads\"],\n",
    "    n_blocks=config[\"n_blocks\"],\n",
    "    d_ff=config[\"d_ff\"],\n",
    ")\n",
    "model.cuda().train()\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-9)\n",
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.48658561706543\n",
      "9.852306365966797\n",
      "9.777769088745117\n",
      "9.666818618774414\n",
      "9.666036605834961\n",
      "9.29448127746582\n",
      "9.456995010375977\n",
      "9.34490966796875\n",
      "9.43882942199707\n",
      "9.111284255981445\n",
      "9.216217041015625\n",
      "8.905555725097656\n",
      "8.746499061584473\n",
      "9.016515731811523\n",
      "9.037217140197754\n",
      "8.893003463745117\n",
      "8.916192054748535\n",
      "8.953568458557129\n",
      "8.764799118041992\n",
      "9.039119720458984\n",
      "8.509190559387207\n",
      "8.440814971923828\n",
      "8.660299301147461\n",
      "8.428574562072754\n",
      "8.796348571777344\n",
      "8.523812294006348\n",
      "8.573486328125\n",
      "8.465386390686035\n",
      "8.551826477050781\n",
      "8.498612403869629\n",
      "8.760406494140625\n",
      "8.235589027404785\n",
      "8.483545303344727\n",
      "8.360697746276855\n",
      "8.234393119812012\n",
      "8.102263450622559\n",
      "8.315869331359863\n",
      "8.157088279724121\n",
      "8.346614837646484\n",
      "8.317895889282227\n",
      "7.871866226196289\n",
      "8.167115211486816\n",
      "7.770043849945068\n",
      "7.839936256408691\n",
      "8.155109405517578\n",
      "7.991952896118164\n",
      "8.125548362731934\n",
      "8.014846801757812\n",
      "7.817283630371094\n",
      "8.139261245727539\n",
      "8.082860946655273\n",
      "7.735185623168945\n",
      "7.600800514221191\n",
      "8.039831161499023\n",
      "8.015372276306152\n",
      "7.891376495361328\n",
      "7.644134044647217\n",
      "7.717573165893555\n",
      "7.803282737731934\n",
      "7.779310703277588\n",
      "7.8403754234313965\n",
      "8.137785911560059\n",
      "7.714184284210205\n",
      "7.9473772048950195\n",
      "7.525972366333008\n",
      "7.86991024017334\n",
      "7.467708110809326\n",
      "8.047869682312012\n",
      "7.5914764404296875\n",
      "7.471780776977539\n",
      "7.555998802185059\n",
      "7.907575607299805\n",
      "7.628215789794922\n",
      "7.885254383087158\n",
      "7.607378005981445\n",
      "7.771159648895264\n",
      "7.435286521911621\n",
      "7.466436862945557\n",
      "7.905431270599365\n",
      "7.177225589752197\n",
      "7.797248840332031\n",
      "7.409952640533447\n",
      "7.4693827629089355\n",
      "7.7088141441345215\n",
      "7.749227523803711\n",
      "7.541913986206055\n",
      "7.490806579589844\n",
      "7.411922931671143\n",
      "7.44686222076416\n",
      "7.569521903991699\n",
      "7.607353687286377\n",
      "7.32640266418457\n",
      "7.501055717468262\n",
      "7.642043590545654\n",
      "7.467929363250732\n",
      "7.722296237945557\n",
      "7.506763458251953\n",
      "7.231316089630127\n",
      "7.206982612609863\n",
      "7.93373966217041\n",
      "7.461167335510254\n",
      "7.4893646240234375\n",
      "7.406795978546143\n",
      "7.537798881530762\n",
      "7.310328483581543\n",
      "6.893813133239746\n",
      "7.761388778686523\n",
      "7.272261619567871\n",
      "7.526917934417725\n",
      "8.105243682861328\n",
      "7.096923351287842\n",
      "7.702207565307617\n",
      "7.556443214416504\n",
      "7.479201793670654\n",
      "7.694014072418213\n",
      "7.365378379821777\n",
      "7.533373832702637\n",
      "7.686572074890137\n",
      "7.3443169593811035\n",
      "7.818673610687256\n",
      "7.062864303588867\n",
      "7.350215911865234\n",
      "7.729750156402588\n",
      "7.496038913726807\n",
      "7.434159278869629\n",
      "7.748493671417236\n",
      "7.363039016723633\n",
      "7.530837535858154\n",
      "7.482542991638184\n",
      "7.499264717102051\n",
      "7.674504280090332\n",
      "7.476540565490723\n",
      "7.603894233703613\n",
      "7.327895164489746\n",
      "7.033961296081543\n",
      "7.547780990600586\n",
      "7.501866340637207\n",
      "7.462234020233154\n",
      "7.5482354164123535\n",
      "7.349971771240234\n",
      "7.668996334075928\n",
      "7.370314598083496\n",
      "7.090419769287109\n",
      "7.205516338348389\n",
      "7.738251686096191\n",
      "7.047857284545898\n",
      "7.284745216369629\n",
      "7.573601245880127\n",
      "7.423479080200195\n",
      "7.198049545288086\n",
      "7.298651695251465\n",
      "7.399956703186035\n",
      "7.097513675689697\n",
      "7.490450382232666\n",
      "7.305351734161377\n",
      "7.647150039672852\n",
      "7.123673439025879\n",
      "7.554783821105957\n",
      "7.2796759605407715\n",
      "7.723134517669678\n",
      "7.187873840332031\n",
      "7.531731605529785\n",
      "7.252722263336182\n",
      "7.338772773742676\n",
      "7.351615905761719\n",
      "7.628109931945801\n",
      "7.431802749633789\n",
      "7.223247051239014\n",
      "6.9423065185546875\n",
      "6.970427513122559\n",
      "7.284521579742432\n",
      "7.678606986999512\n",
      "7.0711164474487305\n",
      "7.2301836013793945\n",
      "6.913193702697754\n",
      "7.134369850158691\n",
      "7.19351053237915\n",
      "7.1577348709106445\n",
      "7.751086711883545\n",
      "7.845122337341309\n",
      "7.326526641845703\n",
      "7.285428524017334\n",
      "7.491874694824219\n",
      "6.9897918701171875\n",
      "7.534828186035156\n",
      "7.3102874755859375\n",
      "7.517446041107178\n",
      "7.329136848449707\n",
      "7.148854732513428\n",
      "7.142783164978027\n",
      "7.476413249969482\n",
      "7.538976669311523\n",
      "7.577512264251709\n",
      "7.459653377532959\n",
      "7.693600177764893\n",
      "7.046119689941406\n",
      "7.320744037628174\n",
      "6.888077259063721\n",
      "7.4304518699646\n",
      "7.360993385314941\n",
      "7.397887706756592\n",
      "7.601720333099365\n",
      "7.322930335998535\n",
      "7.055769920349121\n",
      "7.181188583374023\n",
      "7.529894828796387\n",
      "7.2076921463012695\n",
      "7.4193925857543945\n",
      "7.38615608215332\n",
      "7.473433971405029\n",
      "7.549080848693848\n",
      "7.522216796875\n",
      "7.121096611022949\n",
      "7.533255577087402\n",
      "7.191786289215088\n",
      "7.540407180786133\n",
      "7.17604398727417\n",
      "7.174724578857422\n",
      "7.372556686401367\n",
      "7.102653980255127\n",
      "7.4728217124938965\n",
      "7.673457145690918\n",
      "7.413143157958984\n",
      "6.982466697692871\n",
      "7.49505615234375\n",
      "7.502802848815918\n",
      "7.428406238555908\n",
      "7.102933406829834\n",
      "6.96664571762085\n",
      "7.654515743255615\n",
      "7.080408096313477\n",
      "7.375190258026123\n",
      "7.242545127868652\n",
      "7.508147716522217\n",
      "7.0992560386657715\n",
      "7.5716352462768555\n",
      "7.3321638107299805\n",
      "7.436347961425781\n",
      "6.829098701477051\n",
      "6.889290809631348\n",
      "6.963116645812988\n",
      "7.388888359069824\n",
      "7.4163818359375\n",
      "7.045127868652344\n",
      "7.351287841796875\n",
      "7.497779369354248\n",
      "7.342851161956787\n",
      "7.050042152404785\n",
      "7.088196277618408\n",
      "7.2753071784973145\n",
      "7.240361213684082\n",
      "7.69565486907959\n",
      "7.231183052062988\n",
      "7.63731050491333\n",
      "7.340277671813965\n",
      "7.483403205871582\n",
      "6.830667972564697\n",
      "7.0990095138549805\n",
      "6.942058563232422\n",
      "7.31181001663208\n",
      "7.189868927001953\n",
      "7.117714881896973\n",
      "6.930924415588379\n",
      "7.302491188049316\n",
      "7.089238166809082\n",
      "6.79019832611084\n",
      "7.211517810821533\n",
      "7.122722148895264\n",
      "7.100820064544678\n",
      "7.16198205947876\n",
      "7.446287631988525\n",
      "7.302128791809082\n",
      "7.147316932678223\n",
      "7.359607696533203\n",
      "7.051252365112305\n",
      "7.1871771812438965\n",
      "6.957731246948242\n",
      "7.227618217468262\n",
      "7.069265365600586\n",
      "7.253298282623291\n",
      "6.844170570373535\n",
      "7.260804653167725\n",
      "7.342649459838867\n",
      "7.117287635803223\n",
      "7.256048679351807\n",
      "7.66054630279541\n",
      "7.376094818115234\n",
      "7.4254231452941895\n",
      "6.693731784820557\n",
      "7.29032039642334\n",
      "7.281309127807617\n",
      "7.277826309204102\n",
      "7.079102516174316\n",
      "7.418924331665039\n",
      "6.796257495880127\n",
      "7.62255334854126\n",
      "7.316194534301758\n",
      "6.865847587585449\n",
      "6.8877787590026855\n",
      "7.090116500854492\n",
      "7.277010917663574\n",
      "7.1241044998168945\n",
      "7.008004188537598\n",
      "7.2509331703186035\n",
      "6.626526832580566\n",
      "6.638461589813232\n",
      "7.297522068023682\n",
      "6.740628242492676\n",
      "7.086087226867676\n",
      "7.173155307769775\n",
      "6.848179340362549\n",
      "7.374298095703125\n",
      "7.363150119781494\n",
      "7.154857158660889\n",
      "7.2248430252075195\n",
      "6.917745590209961\n",
      "6.735021591186523\n",
      "7.231805324554443\n",
      "6.9136576652526855\n",
      "7.217465877532959\n",
      "7.011933326721191\n",
      "7.358883857727051\n",
      "7.721148490905762\n",
      "6.693854331970215\n",
      "6.837658405303955\n",
      "7.377864360809326\n",
      "6.809333324432373\n",
      "7.068586826324463\n",
      "7.25685977935791\n",
      "7.015883445739746\n",
      "6.868194103240967\n",
      "6.765347003936768\n",
      "7.37328577041626\n",
      "6.717626571655273\n",
      "7.386215686798096\n",
      "6.519994258880615\n",
      "6.969881534576416\n",
      "7.350158214569092\n",
      "7.041164398193359\n",
      "6.924117088317871\n",
      "6.93517541885376\n",
      "7.234706878662109\n",
      "7.132556915283203\n",
      "7.144806861877441\n",
      "7.100964069366455\n",
      "7.1635661125183105\n",
      "6.954534530639648\n",
      "7.0110087394714355\n",
      "7.144726276397705\n",
      "7.475277900695801\n",
      "7.278141975402832\n",
      "7.182926177978516\n",
      "7.140198230743408\n",
      "6.655203342437744\n",
      "7.132731914520264\n",
      "7.161842346191406\n",
      "6.859940528869629\n",
      "7.177423477172852\n",
      "7.067584991455078\n",
      "6.934760093688965\n",
      "7.207636833190918\n",
      "7.251818656921387\n",
      "7.107188701629639\n",
      "7.2278313636779785\n",
      "7.108872413635254\n",
      "6.34049654006958\n",
      "6.776948928833008\n",
      "7.342715263366699\n",
      "7.4325270652771\n",
      "7.359436511993408\n",
      "6.590956687927246\n",
      "7.1362409591674805\n",
      "7.026586055755615\n",
      "7.101428985595703\n",
      "7.359467506408691\n",
      "7.2642107009887695\n",
      "6.895103454589844\n",
      "6.873472690582275\n",
      "6.756078720092773\n",
      "6.814126014709473\n",
      "7.103142738342285\n",
      "6.931545257568359\n",
      "7.189245223999023\n",
      "7.10795259475708\n",
      "7.2463812828063965\n",
      "7.194568634033203\n",
      "6.850038051605225\n",
      "7.043348789215088\n",
      "7.311069011688232\n",
      "7.3075480461120605\n",
      "7.404206275939941\n",
      "6.931260585784912\n",
      "6.732951641082764\n",
      "7.500716686248779\n",
      "7.2043843269348145\n",
      "6.732789516448975\n",
      "7.053474426269531\n",
      "7.095137596130371\n",
      "6.9984588623046875\n",
      "6.62025260925293\n",
      "6.644542694091797\n",
      "7.093198299407959\n",
      "6.8425703048706055\n",
      "6.958478927612305\n",
      "6.818131446838379\n",
      "7.105340957641602\n",
      "6.756587505340576\n",
      "6.992390155792236\n",
      "6.914887428283691\n",
      "7.0839924812316895\n",
      "6.940038204193115\n",
      "7.3519287109375\n",
      "6.864599227905273\n",
      "6.803050994873047\n",
      "6.952323913574219\n",
      "7.106025695800781\n",
      "7.162842750549316\n",
      "6.921053886413574\n",
      "6.825864315032959\n",
      "6.4034743309021\n",
      "7.2642412185668945\n",
      "6.967126846313477\n",
      "6.719017028808594\n",
      "6.991524696350098\n",
      "6.958340167999268\n",
      "6.977419376373291\n",
      "7.131528854370117\n",
      "7.123093605041504\n",
      "6.911499977111816\n",
      "7.070387840270996\n",
      "7.014043807983398\n",
      "6.616451263427734\n",
      "7.044776916503906\n",
      "7.077925682067871\n",
      "7.0962724685668945\n",
      "7.268314838409424\n",
      "6.754818439483643\n",
      "7.016163349151611\n",
      "6.663052558898926\n",
      "6.96345853805542\n",
      "7.065088748931885\n",
      "7.090368270874023\n",
      "7.021111488342285\n",
      "7.246500015258789\n",
      "6.897677898406982\n",
      "7.0280609130859375\n",
      "6.656573295593262\n",
      "6.879950523376465\n",
      "7.060837745666504\n",
      "6.754794597625732\n",
      "7.142873287200928\n",
      "6.937872886657715\n",
      "6.806560039520264\n",
      "7.007040977478027\n",
      "6.733246803283691\n",
      "6.354619979858398\n",
      "6.8424153327941895\n",
      "6.970562934875488\n",
      "6.875915050506592\n",
      "7.193347930908203\n",
      "7.275537014007568\n",
      "7.499690055847168\n",
      "6.8483991622924805\n",
      "7.332569122314453\n",
      "7.138406753540039\n",
      "7.018510341644287\n",
      "6.969631671905518\n",
      "6.882747173309326\n",
      "6.917686939239502\n",
      "7.100577354431152\n",
      "6.825542449951172\n",
      "6.77569580078125\n",
      "6.436022758483887\n",
      "6.639451026916504\n",
      "6.847832679748535\n",
      "7.122719764709473\n",
      "6.994760513305664\n",
      "6.909204483032227\n",
      "7.016907215118408\n",
      "7.1626691818237305\n",
      "6.307103157043457\n",
      "6.652386665344238\n",
      "6.820629596710205\n",
      "6.694863319396973\n",
      "6.650225639343262\n",
      "7.608067989349365\n",
      "7.068929672241211\n",
      "7.114619731903076\n",
      "6.6095871925354\n",
      "6.665961742401123\n",
      "7.2537312507629395\n",
      "7.162617206573486\n",
      "6.465794086456299\n",
      "6.721641540527344\n",
      "6.579404830932617\n",
      "6.611481189727783\n",
      "7.519781112670898\n",
      "6.338984489440918\n",
      "7.121592998504639\n",
      "6.561023235321045\n",
      "6.976517677307129\n",
      "6.913776397705078\n",
      "6.8584465980529785\n",
      "6.836686611175537\n",
      "6.734349250793457\n",
      "6.440853595733643\n",
      "7.01416015625\n",
      "6.854699611663818\n",
      "7.205841541290283\n",
      "6.443441867828369\n",
      "6.8575825691223145\n",
      "7.104442596435547\n",
      "7.052985191345215\n",
      "6.277730464935303\n",
      "7.350381374359131\n",
      "7.131171226501465\n",
      "7.054101467132568\n",
      "7.1825852394104\n",
      "6.903197765350342\n",
      "7.129415512084961\n",
      "6.933725357055664\n",
      "7.016499042510986\n",
      "6.800519943237305\n",
      "7.216084003448486\n",
      "6.986865520477295\n",
      "7.071000099182129\n",
      "6.500936508178711\n",
      "6.692607879638672\n",
      "6.858560562133789\n",
      "6.744268894195557\n",
      "6.8157429695129395\n",
      "6.6316094398498535\n",
      "7.200850486755371\n",
      "6.649730682373047\n",
      "6.8701372146606445\n",
      "7.007157325744629\n",
      "6.9005889892578125\n",
      "7.045724391937256\n",
      "6.877871513366699\n",
      "6.718181610107422\n",
      "7.025079727172852\n",
      "6.429255485534668\n",
      "6.880663871765137\n",
      "7.100414752960205\n",
      "6.9802775382995605\n",
      "6.6367082595825195\n",
      "6.7868452072143555\n",
      "7.096550941467285\n",
      "6.778446674346924\n",
      "6.540005683898926\n",
      "7.309434413909912\n",
      "7.23467493057251\n",
      "7.229861259460449\n",
      "7.184873104095459\n",
      "6.674396514892578\n",
      "6.843078136444092\n",
      "6.768791198730469\n",
      "6.954346656799316\n",
      "7.073585510253906\n",
      "7.12200403213501\n",
      "7.114649772644043\n",
      "7.196190357208252\n",
      "6.67376708984375\n",
      "6.699810028076172\n",
      "6.779257297515869\n",
      "7.0199666023254395\n",
      "6.961990833282471\n",
      "7.036627769470215\n",
      "6.4495086669921875\n",
      "7.046795845031738\n",
      "6.892631530761719\n",
      "6.81727933883667\n",
      "6.738321781158447\n",
      "7.104060649871826\n",
      "7.184463024139404\n",
      "6.722362041473389\n",
      "6.602651119232178\n",
      "6.838265419006348\n",
      "6.826433181762695\n",
      "6.938314914703369\n",
      "6.8415961265563965\n",
      "6.418991565704346\n",
      "6.500549793243408\n",
      "6.496718406677246\n",
      "7.102914810180664\n",
      "7.293337345123291\n",
      "7.103865146636963\n",
      "6.269493103027344\n",
      "6.987987995147705\n",
      "7.114490509033203\n",
      "6.866621971130371\n",
      "6.689861297607422\n",
      "7.252468109130859\n",
      "6.810567855834961\n",
      "7.1101813316345215\n",
      "6.919564247131348\n",
      "7.149294853210449\n",
      "6.634040832519531\n",
      "6.77516508102417\n",
      "7.180794715881348\n",
      "6.920250415802002\n",
      "6.654083251953125\n",
      "6.861693382263184\n",
      "6.589870452880859\n",
      "7.089351654052734\n",
      "6.442744731903076\n",
      "6.697309494018555\n",
      "6.929895401000977\n",
      "6.984705448150635\n",
      "6.589505195617676\n",
      "6.742877960205078\n",
      "6.96277379989624\n",
      "7.155439853668213\n",
      "6.99290657043457\n",
      "6.920333385467529\n",
      "6.699237823486328\n",
      "7.066625118255615\n",
      "6.718735218048096\n",
      "6.894944190979004\n",
      "6.318268299102783\n",
      "6.716040134429932\n",
      "6.992868900299072\n",
      "6.667984962463379\n",
      "6.703255653381348\n",
      "7.24410343170166\n",
      "6.92458963394165\n",
      "7.217638969421387\n",
      "6.831180095672607\n",
      "7.071622371673584\n",
      "7.201824188232422\n",
      "7.10134220123291\n",
      "6.4809417724609375\n",
      "6.755668640136719\n",
      "6.78607177734375\n",
      "6.727482318878174\n",
      "6.401630878448486\n",
      "7.085434436798096\n",
      "6.606167316436768\n",
      "6.595005512237549\n",
      "6.923404216766357\n",
      "7.435662746429443\n",
      "6.671053886413574\n",
      "7.087385177612305\n",
      "6.682130336761475\n",
      "6.988944053649902\n",
      "6.9630842208862305\n",
      "6.687208652496338\n",
      "6.7680230140686035\n",
      "7.266739845275879\n",
      "6.5134501457214355\n",
      "7.137530326843262\n",
      "6.709896087646484\n",
      "7.023486137390137\n",
      "6.3959431648254395\n",
      "6.986067771911621\n",
      "7.073633193969727\n",
      "6.861861228942871\n",
      "6.9329071044921875\n",
      "6.9568586349487305\n",
      "6.853738307952881\n",
      "6.8717851638793945\n",
      "6.78816032409668\n",
      "6.941685676574707\n",
      "6.686064720153809\n",
      "6.689033508300781\n",
      "6.778569221496582\n",
      "6.969429016113281\n",
      "6.8180251121521\n",
      "6.726872444152832\n",
      "7.075326919555664\n",
      "6.883764266967773\n",
      "6.661483287811279\n",
      "7.21921443939209\n",
      "6.812951564788818\n",
      "7.117401123046875\n",
      "6.717820644378662\n",
      "6.836991310119629\n",
      "6.6736578941345215\n",
      "6.800184726715088\n",
      "6.863982200622559\n",
      "6.875940799713135\n",
      "6.414055824279785\n",
      "6.345643520355225\n",
      "6.46025276184082\n",
      "6.433891773223877\n",
      "7.0723490715026855\n",
      "6.882568359375\n",
      "6.924225807189941\n",
      "6.937612056732178\n",
      "6.448580741882324\n",
      "7.185329437255859\n",
      "6.856112480163574\n",
      "6.7425994873046875\n",
      "6.913817405700684\n",
      "7.113043785095215\n",
      "6.653280258178711\n",
      "6.939945220947266\n",
      "6.732122421264648\n",
      "6.917989730834961\n",
      "6.881959915161133\n",
      "6.7868218421936035\n",
      "6.355586051940918\n",
      "6.745831489562988\n",
      "6.5602617263793945\n",
      "6.453085899353027\n",
      "6.772252082824707\n",
      "6.975215911865234\n",
      "7.074366092681885\n",
      "6.57927131652832\n",
      "6.793550491333008\n",
      "6.759196758270264\n",
      "6.871082782745361\n",
      "6.76516580581665\n",
      "6.908534049987793\n",
      "6.777112007141113\n",
      "5.993743896484375\n",
      "6.9599609375\n",
      "6.847177505493164\n",
      "7.203678607940674\n",
      "6.538673400878906\n",
      "6.819095611572266\n",
      "6.205696105957031\n",
      "6.807628631591797\n",
      "6.916017532348633\n",
      "6.242167949676514\n",
      "7.165940761566162\n",
      "6.837294101715088\n",
      "6.709673881530762\n",
      "6.157525062561035\n",
      "6.923693656921387\n",
      "6.598485946655273\n",
      "6.026533603668213\n",
      "6.5723419189453125\n",
      "6.914234161376953\n",
      "6.9319281578063965\n",
      "7.099289894104004\n",
      "6.893579006195068\n",
      "6.835049152374268\n",
      "6.208431720733643\n",
      "7.089500904083252\n",
      "6.861076354980469\n",
      "6.870368003845215\n",
      "7.039216041564941\n",
      "6.598352909088135\n",
      "6.390336513519287\n",
      "6.76718282699585\n",
      "6.682011604309082\n",
      "6.99938440322876\n",
      "6.665493011474609\n",
      "6.426151275634766\n",
      "6.900234222412109\n",
      "6.823811054229736\n",
      "6.891501426696777\n",
      "6.477503776550293\n",
      "6.579565525054932\n",
      "6.905850887298584\n",
      "6.669018745422363\n",
      "6.458596229553223\n",
      "6.618048667907715\n",
      "6.411596298217773\n",
      "6.5262603759765625\n",
      "6.787761688232422\n",
      "7.032245635986328\n",
      "6.642571926116943\n",
      "6.676517009735107\n",
      "6.785097122192383\n",
      "6.807677268981934\n",
      "7.028209686279297\n",
      "6.7084126472473145\n",
      "6.654809951782227\n",
      "6.773556232452393\n",
      "6.275760173797607\n",
      "6.6408772468566895\n",
      "6.4666056632995605\n",
      "6.695537567138672\n",
      "7.199679374694824\n",
      "6.915200233459473\n",
      "7.2438836097717285\n",
      "7.282218933105469\n",
      "6.720518589019775\n",
      "6.312890529632568\n",
      "6.674749851226807\n",
      "7.216145992279053\n",
      "7.068466663360596\n",
      "6.858767509460449\n",
      "6.617858409881592\n",
      "6.402526378631592\n",
      "6.909713268280029\n",
      "6.414444923400879\n",
      "7.124168395996094\n",
      "6.340363025665283\n",
      "7.0434794425964355\n",
      "6.590610504150391\n",
      "6.73615837097168\n",
      "6.5463104248046875\n",
      "5.961850166320801\n",
      "6.870647430419922\n",
      "7.151150226593018\n",
      "7.2199225425720215\n",
      "6.695516586303711\n",
      "6.831707000732422\n",
      "6.899252891540527\n",
      "6.867611885070801\n",
      "6.954773902893066\n",
      "6.977602005004883\n",
      "6.649693489074707\n",
      "6.3436503410339355\n",
      "6.737631797790527\n",
      "6.420461177825928\n",
      "6.51137113571167\n",
      "6.7701735496521\n",
      "6.493671417236328\n",
      "6.9497270584106445\n",
      "6.71021842956543\n",
      "7.029531478881836\n",
      "7.089685440063477\n",
      "6.740269184112549\n",
      "7.092363357543945\n",
      "6.774773597717285\n",
      "6.605656147003174\n",
      "6.680632591247559\n",
      "6.713156700134277\n",
      "6.398575782775879\n",
      "6.750248908996582\n",
      "6.788824081420898\n",
      "6.368199825286865\n",
      "6.616237640380859\n",
      "6.8289337158203125\n",
      "7.047356605529785\n",
      "6.9890055656433105\n",
      "6.7684478759765625\n",
      "6.7136430740356445\n",
      "6.547901630401611\n",
      "6.387107849121094\n",
      "7.118574142456055\n",
      "6.491636753082275\n",
      "6.894243240356445\n",
      "6.517595291137695\n",
      "6.618443489074707\n",
      "6.458564758300781\n",
      "5.921009540557861\n",
      "7.069981575012207\n",
      "6.727685451507568\n",
      "7.0494303703308105\n",
      "6.608926773071289\n",
      "6.871427059173584\n",
      "6.779543876647949\n",
      "7.007999897003174\n",
      "6.482233047485352\n",
      "6.78610897064209\n",
      "6.494866371154785\n",
      "6.8097028732299805\n",
      "6.494619846343994\n",
      "6.428513526916504\n",
      "6.657532691955566\n",
      "6.4331207275390625\n",
      "6.729223251342773\n",
      "6.72443962097168\n",
      "7.031973838806152\n",
      "6.7545270919799805\n",
      "6.876903057098389\n",
      "6.4674224853515625\n",
      "6.703035831451416\n",
      "7.125217437744141\n",
      "7.030374526977539\n",
      "6.658792018890381\n",
      "6.730771064758301\n",
      "6.324026107788086\n",
      "6.567470550537109\n",
      "6.864056587219238\n",
      "7.0202436447143555\n",
      "6.427639007568359\n",
      "6.552183628082275\n",
      "6.3511962890625\n",
      "6.804099082946777\n",
      "6.807331085205078\n",
      "6.688374996185303\n",
      "6.372039318084717\n",
      "6.81053352355957\n",
      "6.209609031677246\n",
      "6.9716949462890625\n",
      "6.822421550750732\n",
      "6.684202671051025\n",
      "6.966054916381836\n",
      "7.0062479972839355\n",
      "7.007001876831055\n",
      "6.684395790100098\n",
      "6.721426963806152\n",
      "6.338522911071777\n",
      "6.448703765869141\n",
      "7.096432685852051\n",
      "7.014410972595215\n",
      "6.549472808837891\n",
      "6.545680999755859\n",
      "6.811451435089111\n",
      "6.389642715454102\n",
      "6.844880104064941\n",
      "6.8662567138671875\n",
      "6.919579982757568\n",
      "6.547303676605225\n",
      "6.517186164855957\n",
      "7.066859245300293\n",
      "6.660078525543213\n",
      "6.900997161865234\n",
      "6.820692539215088\n",
      "6.326971054077148\n",
      "6.603440761566162\n",
      "6.823633670806885\n",
      "6.79206657409668\n",
      "6.901628494262695\n",
      "6.628647804260254\n",
      "6.1799516677856445\n",
      "6.753605365753174\n",
      "6.938150405883789\n",
      "6.804678440093994\n",
      "6.5750627517700195\n",
      "6.128629207611084\n",
      "6.806209087371826\n",
      "6.806652069091797\n",
      "6.821799278259277\n",
      "6.622746467590332\n",
      "6.780854225158691\n",
      "6.642996788024902\n",
      "6.488085746765137\n",
      "6.855140209197998\n",
      "6.956035614013672\n",
      "6.434144496917725\n",
      "6.869128704071045\n",
      "6.7164812088012695\n",
      "6.953854084014893\n",
      "6.822202205657959\n",
      "6.439448356628418\n",
      "6.7649149894714355\n",
      "7.093070983886719\n",
      "6.476019859313965\n",
      "6.418011665344238\n",
      "6.790496826171875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m----> 4\u001b[0m         encoder_input \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, seq_len)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (B, seq_len)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(input_encoder\u001b[38;5;241m=\u001b[39mencoder_input, input_decoder\u001b[38;5;241m=\u001b[39mdecoder_input)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    for batch in train_dataloader:\n",
    "        encoder_input = batch[\"encoder_input\"].to(device)  # (b, seq_len)\n",
    "        decoder_input = batch[\"decoder_input\"].to(device)  # (B, seq_len)\n",
    "        out = model.forward(input_encoder=encoder_input, input_decoder=decoder_input)\n",
    "        # out is (batch_size, seq_len, vocab_size)\n",
    "        # We want to compute the loss for each token in the sequence, so we reshape\n",
    "        # to (batch_size * seq_len, vocab_size)\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        # batch[\"label\"] is (batch_size, seq_len)\n",
    "        # We reshape it to (batch_size * seq_len)\n",
    "        labels = batch[\"label\"].to(device).reshape(-1)\n",
    "        loss = loss_fn(out, labels)  # Removed vocab_idxs, and reshaped out and labels\n",
    "        print(loss.item())\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
