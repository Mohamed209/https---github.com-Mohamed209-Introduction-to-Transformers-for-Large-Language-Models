{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the architecture from (Attention Is All You Need) https://arxiv.org/abs/1706.03762\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic architecture \n",
    "\n",
    "![image.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12cdf506-6cd8-4afa-93a3-b77b82770309_2755x1570.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.gif](https://i.imgur.com/KgZCdzX.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical way to implement the values of the embedding is by hard coding them by using a sine and cosine function of the vectors and elementsâ€™ positions\n",
    "\n",
    "![image.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a77f49-ed6d-4614-9c64-505455bd0c83_2043x1300.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_size: int, d_attn: int):\n",
    "        \"\"\"represent positional encoding as harcoded matrix of size (context_size,d_attn)\n",
    "\n",
    "        Args:\n",
    "            context_size (int): max context size\n",
    "            d_attn (int): model hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(\n",
    "            size=(context_size, d_attn)\n",
    "        )  # placeholder matrix of the encoding , check above figures (orange matrix)\n",
    "        pos = torch.arange(0, context_size).unsqueeze(\n",
    "            dim=1\n",
    "        )  # positions are ranged from 0 to context size (those are rows indexes in orange matrix in above figures)\n",
    "        i = torch.arange(\n",
    "            0, d_attn, 2\n",
    "        )  # i range from 0 to d_attn in every pos (row in orange matrix)\n",
    "        arg = pos / (10000 ** (2 * i / d_attn))\n",
    "        self.encoding[:, 0::2] = torch.sin(arg)  # even columns (even i)\n",
    "        self.encoding[:, 1::2] = torch.cos(arg)  # odd i\n",
    "\n",
    "    def forward(self, tokens_sequence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"encode embedded tokens sequence\n",
    "\n",
    "        Args:\n",
    "            tokens_sequence (torch.Tensor):\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: position encoded embedded tokens\n",
    "        \"\"\"\n",
    "        return self.encoding[\n",
    "            : tokens_sequence.shape[1], :\n",
    "        ]  # just query the self.encoding matrix with tokens sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder block is composed of a multi-head attention layer, a position-wise feed-forward network, and two-layer normalization.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6627a678-0582-4950-a829-a8e9e4e97db9_3289x1326.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention layer allows to learn complex relationships between the hidden states, whereas the position-wise feed-forward network allows to learn complex relationships between the different elements within each vector.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf4bdbd2-8e45-4f33-9c86-35eede3571ab_3433x1050.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_attn, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_attn, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_attn)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_attention_heads: int, d_ff: int, d_attn: int):\n",
    "        \"\"\"init encoder\n",
    "\n",
    "        Args:\n",
    "            n_attention_heads (int): number of attention heads\n",
    "            d_ff (int): dimention feed forward network\n",
    "            d_attn (int): encoder hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_attn, n_attention_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_attn, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_attn)\n",
    "        self.norm2 = nn.LayerNorm(d_attn)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): hidden state tensor is elementwise addition between token embeddings and positional embeddings for the input sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: encoder projection tensor (encoder output)\n",
    "        \"\"\"\n",
    "        out1 = (\n",
    "            self.self_attn(query=hidden_states, key=hidden_states, value=hidden_states)[\n",
    "                0\n",
    "            ]\n",
    "            + hidden_states  # apply resiudal connection\n",
    "        )  # perform self attention on hidden states (note hidden state tensor is elementwise addition between token embeddings and positional embeddings)\n",
    "        norm1 = self.norm1(out1)  # layer normalization\n",
    "        out2 = self.feed_forward(norm1) + norm1\n",
    "        out3 = self.norm2(out2)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is just the token embedding and the position embedding followed by multiple encoder blocks.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3808c9f-715e-4ab0-be11-34e16b3d8644_3540x1022.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        context_size: int,\n",
    "        n_blocks: int,\n",
    "        n_heads: int,\n",
    "        d_attn: int,\n",
    "        d_ff: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_attn)\n",
    "        self.pos_embedding = PositionalEncoding(context_size, d_attn)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    d_attn=d_attn,\n",
    "                    n_attention_heads=n_heads,\n",
    "                    d_ff=d_ff,\n",
    "                )\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens_seq: torch.Tensor) -> torch.Tensor:\n",
    "        embedded_tokens = self.embedding(\n",
    "            tokens_seq\n",
    "        )  # apply embeddings layer to tokens input sequence\n",
    "        pos_embedded_tokens = self.pos_embedding(tokens_seq)\n",
    "        hidden_states = embedded_tokens + pos_embedded_tokens\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Block\n",
    "\n",
    "The decoder block is composed of a multi-head attention layer, a position-wise feed-forward network, a cross-attention layer, and three layer normalization.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0287aa3-7a69-41c4-a692-c1940e007f29_3301x1582.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_attn: int, num_heads: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_attn, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_attn)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_attn, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_attn)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_attn, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_attn)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, encoder_output: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): hidden state tensor is elementwise addition between token embeddings and positional embeddings for the output sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        out1 = (\n",
    "            self.self_attn(query=hidden_states, key=hidden_states, value=hidden_states)[\n",
    "                0\n",
    "            ]\n",
    "            + hidden_states\n",
    "        )  # apply resiudal connection\n",
    "        out1 = self.norm1(out1)\n",
    "        # apply corss attention between out1 and encoder output\n",
    "        out2 = (\n",
    "            self.cross_attn(query=out1, key=encoder_output, value=encoder_output)[0]\n",
    "            + out1\n",
    "        )\n",
    "        out2 = self.norm2(out2)\n",
    "        out3 = self.feed_forward(out2) + out2\n",
    "        out3 = self.norm3(out3)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cross-attention layer computes the attentions between the decoder's hidden states and the encoder output\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4f653-3985-40ac-932d-3eb023be2eb0_2723x1332.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is just the token embedding and the position embedding followed by multiple decoder blocks and the predicting head.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0808f5de-f713-4a56-8750-ac1cda39b929_2753x1542.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicting head is just a linear layer that projects the last hidden states from the d_attn dimension to the size of the vocabulary. To predict, we perform an ArgMax function on the resulting probability vectors\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc452d478-581f-4baf-941f-0ab07a39bdb3_3386x1342.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, context_size, d_attn, d_ff, num_heads, n_blocks):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, d_attn)\n",
    "        self.pos_embedding = PositionalEncoding(context_size, d_attn)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    d_attn=d_attn,\n",
    "                    num_heads=num_heads,\n",
    "                    d_ff=d_ff,\n",
    "                )\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.out = nn.Linear(d_attn, output_size)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        x = self.embedding(x) + self.pos_embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_output)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_vocab_size,\n",
    "        decoder_vocab_size,\n",
    "        context_size,\n",
    "        d_attn,\n",
    "        d_ff,\n",
    "        num_heads,\n",
    "        n_blocks,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            encoder_vocab_size, context_size, n_blocks, num_heads, d_attn, d_ff\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            decoder_vocab_size, context_size, d_attn, d_ff, num_heads, n_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, input_encoder, input_decoder):\n",
    "        enc_output = self.encoder(input_encoder)\n",
    "        output = self.decoder(input_decoder, enc_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the architecture with dummy data\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "index2words = {SOS_token: \"SOS\", EOS_token: \"EOS\", PAD_token: \"PAD\"}\n",
    "words = \"How are you doing ? I am good and you ?\"\n",
    "words_list = set(words.lower().split(\" \"))\n",
    "for word in words_list:\n",
    "    index2words[len(index2words)] = word\n",
    "\n",
    "words2index = {w: i for i, w in index2words.items()}\n",
    "\n",
    "\n",
    "def convert2tensors(sentence, max_len):\n",
    "    words_list = sentence.lower().split(\" \")\n",
    "    padding = [\"PAD\"] * (max_len - len(words_list))\n",
    "    words_list.extend(padding)\n",
    "    indexes = [words2index[word] for word in words_list]\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "\n",
    "d_attn = 10\n",
    "VOCAB_SIZE = len(words2index)\n",
    "N_BLOCKS = 10\n",
    "D_FF = 20\n",
    "CONTEXT_SIZE = 100\n",
    "NUM_HEADS = 2\n",
    "transformer = Transformer(\n",
    "    encoder_vocab_size=VOCAB_SIZE,\n",
    "    decoder_vocab_size=VOCAB_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    d_attn=d_attn,\n",
    "    d_ff=D_FF,\n",
    "    num_heads=NUM_HEADS,\n",
    "    n_blocks=N_BLOCKS,\n",
    ")\n",
    "input_sentence = \"How are you doing ?\"\n",
    "output_sentence = \"I am good and\"\n",
    "input_encoder = convert2tensors(input_sentence, CONTEXT_SIZE)\n",
    "input_decoder = convert2tensors(output_sentence, CONTEXT_SIZE)\n",
    "output = transformer(input_encoder, input_decoder)\n",
    "_, indexes = output.squeeze().topk(1)\n",
    "index2words[indexes[3].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = (\n",
    "            self.seq_len - len(enc_input_tokens) - 2\n",
    "        )  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * enc_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_blocks\": 8,\n",
    "        \"d_ff\": 1024,\n",
    "        \"datasource\": \"opus_books\",\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizers/tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\",\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[\"translation\"][lang]\n",
    "\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(\n",
    "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "        )\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(\n",
    "        f\"{config['datasource']}\",\n",
    "        f\"{config['lang_src']}-{config['lang_tgt']}\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(\n",
    "        train_ds_raw,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        config[\"lang_src\"],\n",
    "        config[\"lang_tgt\"],\n",
    "        config[\"seq_len\"],\n",
    "    )\n",
    "    val_ds = BilingualDataset(\n",
    "        val_ds_raw,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        config[\"lang_src\"],\n",
    "        config[\"lang_tgt\"],\n",
    "        config[\"seq_len\"],\n",
    "    )\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, batch_size=config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design\n",
      "Device memory: 5.7974853515625 GB\n",
      "Max length of source sentence: 316\n",
      "Max length of target sentence: 287\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(22439, 512)\n",
      "    (pos_embedding): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x EncoderBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(30000, 512)\n",
      "    (pos_embedding): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-7): 8 x DecoderBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (out): Linear(in_features=512, out_features=30000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "config = get_config()\n",
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "    print(\n",
    "        f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\"\n",
    "    )\n",
    "# Make sure the weights folder exists\n",
    "Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = Transformer(\n",
    "    encoder_vocab_size=tokenizer_src.get_vocab_size(),\n",
    "    decoder_vocab_size=tokenizer_tgt.get_vocab_size(),\n",
    "    context_size=config[\"seq_len\"],\n",
    "    d_attn=config[\"d_model\"],\n",
    "    num_heads=config[\"n_heads\"],\n",
    "    n_blocks=config[\"n_blocks\"],\n",
    "    d_ff=config[\"d_ff\"],\n",
    ")\n",
    "model.train()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
