{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the architecture from (Attention Is All You Need) https://arxiv.org/abs/1706.03762\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic architecture \n",
    "\n",
    "![image.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12cdf506-6cd8-4afa-93a3-b77b82770309_2755x1570.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.gif](https://i.imgur.com/KgZCdzX.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical way to implement the values of the embedding is by hard coding them by using a sine and cosine function of the vectors and elementsâ€™ positions\n",
    "\n",
    "![image.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a77f49-ed6d-4614-9c64-505455bd0c83_2043x1300.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_size: int, d_attn: int):\n",
    "        \"\"\"represent positional encoding as harcoded matrix of size (context_size,d_attn)\n",
    "\n",
    "        Args:\n",
    "            context_size (int): max context size\n",
    "            d_attn (int): model hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(\n",
    "            size=(context_size, d_attn)\n",
    "        )  # placeholder matrix of the encoding , check above figures (orange matrix)\n",
    "        pos = torch.arange(0, context_size).unsqueeze(\n",
    "            dim=1\n",
    "        )  # positions are ranged from 0 to context size (those are rows indexes in orange matrix in above figures)\n",
    "        i = torch.arange(\n",
    "            0, d_attn, 2\n",
    "        )  # i range from 0 to d_attn in every pos (row in orange matrix)\n",
    "        arg = pos / (10000 ** (2 * i / d_attn))\n",
    "        self.encoding[:, 0::2] = torch.sin(arg)  # even columns (even i)\n",
    "        self.encoding[:, 1::2] = torch.cos(arg)  # odd i\n",
    "\n",
    "    def forward(self, tokens_sequence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"encode embedded tokens sequence\n",
    "\n",
    "        Args:\n",
    "            tokens_sequence (torch.Tensor):\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: position encoded embedded tokens\n",
    "        \"\"\"\n",
    "        return self.encoding[\n",
    "            : tokens_sequence.shape[1], :\n",
    "        ]  # just query the self.encoding matrix with tokens sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder block is composed of a multi-head attention layer, a position-wise feed-forward network, and two-layer normalization.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6627a678-0582-4950-a829-a8e9e4e97db9_3289x1326.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention layer allows to learn complex relationships between the hidden states, whereas the position-wise feed-forward network allows to learn complex relationships between the different elements within each vector.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf4bdbd2-8e45-4f33-9c86-35eede3571ab_3433x1050.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_attn, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_attn, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_attn)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_attention_heads: int, d_ff: int, d_attn: int):\n",
    "        \"\"\"init encoder\n",
    "\n",
    "        Args:\n",
    "            n_attention_heads (int): number of attention heads\n",
    "            d_ff (int): dimention feed forward network\n",
    "            d_attn (int): encoder hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_attn, n_attention_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_attn, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_attn)\n",
    "        self.norm2 = nn.LayerNorm(d_attn)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): hidden state tensor is elementwise addition between token embeddings and positional embeddings for the input sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: encoder projection tensor (encoder output)\n",
    "        \"\"\"\n",
    "        out1 = (\n",
    "            self.self_attn(query=hidden_states, key=hidden_states, value=hidden_states)[\n",
    "                0\n",
    "            ]\n",
    "            + hidden_states  # apply resiudal connection\n",
    "        )  # perform self attention on hidden states (note hidden state tensor is elementwise addition between token embeddings and positional embeddings)\n",
    "        norm1 = self.norm1(out1)  # layer normalization\n",
    "        out2 = self.feed_forward(norm1) + norm1\n",
    "        out3 = self.norm2(out2)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is just the token embedding and the position embedding followed by multiple encoder blocks.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3808c9f-715e-4ab0-be11-34e16b3d8644_3540x1022.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        context_size: int,\n",
    "        n_blocks: int,\n",
    "        n_heads: int,\n",
    "        d_attn: int,\n",
    "        d_ff: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_attn)\n",
    "        self.pos_embedding = PositionalEncoding(context_size, d_attn)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    d_attn=d_attn,\n",
    "                    n_attention_heads=n_heads,\n",
    "                    d_ff=d_ff,\n",
    "                )\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens_seq: torch.Tensor) -> torch.Tensor:\n",
    "        embedded_tokens = self.embedding(\n",
    "            tokens_seq\n",
    "        )  # apply embeddings layer to tokens input sequence\n",
    "        pos_embedded_tokens = self.pos_embedding(tokens_seq)\n",
    "        hidden_states = embedded_tokens + pos_embedded_tokens\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Block\n",
    "\n",
    "The decoder block is composed of a multi-head attention layer, a position-wise feed-forward network, a cross-attention layer, and three layer normalization.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0287aa3-7a69-41c4-a692-c1940e007f29_3301x1582.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_attn: int, num_heads: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_attn, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_attn)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_attn, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_attn)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_attn, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_attn)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, encoder_output: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): hidden state tensor is elementwise addition between token embeddings and positional embeddings for the output sequence\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        out1 = (\n",
    "            self.self_attn(query=hidden_states, key=hidden_states, value=hidden_states)[\n",
    "                0\n",
    "            ]\n",
    "            + hidden_states\n",
    "        )  # apply resiudal connection\n",
    "        out1 = self.norm1(out1)\n",
    "        # apply corss attention between out1 and encoder output\n",
    "        out2 = (\n",
    "            self.cross_attn(query=out1, key=encoder_output, value=encoder_output)[0]\n",
    "            + out1\n",
    "        )\n",
    "        out2 = self.norm2(out2)\n",
    "        out3 = self.feed_forward(out2) + out2\n",
    "        out3 = self.norm3(out3)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cross-attention layer computes the attentions between the decoder's hidden states and the encoder output\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa4f653-3985-40ac-932d-3eb023be2eb0_2723x1332.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is just the token embedding and the position embedding followed by multiple decoder blocks and the predicting head.\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0808f5de-f713-4a56-8750-ac1cda39b929_2753x1542.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicting head is just a linear layer that projects the last hidden states from the d_attn dimension to the size of the vocabulary. To predict, we perform an ArgMax function on the resulting probability vectors\n",
    "\n",
    "![img.png](https://cdn.fs.teachablecdn.com/ADNupMnWyR7kCWRvm76Laz/https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc452d478-581f-4baf-941f-0ab07a39bdb3_3386x1342.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, context_size, d_attn, d_ff, num_heads, n_blocks):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, d_attn)\n",
    "        self.pos_embedding = PositionalEncoding(context_size, d_attn)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    d_attn=d_attn,\n",
    "                    num_heads=num_heads,\n",
    "                    d_ff=d_ff,\n",
    "                )\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.out = nn.Linear(d_attn, output_size)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        x = self.embedding(x) + self.pos_embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_output)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_attn, d_ff, num_heads, n_blocks):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size, context_size, n_blocks, num_heads, d_attn, d_ff\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size, context_size, d_attn, d_ff, num_heads, n_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, input_encoder, input_decoder):\n",
    "        enc_output = self.encoder(input_encoder)\n",
    "        output = self.decoder(input_decoder, enc_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "index2words = {SOS_token: \"SOS\", EOS_token: \"EOS\", PAD_token: \"PAD\"}\n",
    "words = \"How are you doing ? I am good and you ?\"\n",
    "words_list = set(words.lower().split(\" \"))\n",
    "for word in words_list:\n",
    "    index2words[len(index2words)] = word\n",
    "\n",
    "words2index = {w: i for i, w in index2words.items()}\n",
    "\n",
    "\n",
    "def convert2tensors(sentence, max_len):\n",
    "    words_list = sentence.lower().split(\" \")\n",
    "    padding = [\"PAD\"] * (max_len - len(words_list))\n",
    "    words_list.extend(padding)\n",
    "    indexes = [words2index[word] for word in words_list]\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doing'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_attn = 10\n",
    "VOCAB_SIZE = len(words2index)\n",
    "N_BLOCKS = 10\n",
    "D_FF = 20\n",
    "CONTEXT_SIZE = 100\n",
    "NUM_HEADS = 2\n",
    "transformer = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    d_attn=d_attn,\n",
    "    d_ff=D_FF,\n",
    "    num_heads=NUM_HEADS,\n",
    "    n_blocks=N_BLOCKS,\n",
    ")\n",
    "input_sentence = \"How are you doing ?\"\n",
    "output_sentence = \"I am good and\"\n",
    "input_encoder = convert2tensors(input_sentence, CONTEXT_SIZE)\n",
    "input_decoder = convert2tensors(output_sentence, CONTEXT_SIZE)\n",
    "output = transformer(input_encoder, input_decoder)\n",
    "_, indexes = output.squeeze().topk(1)\n",
    "index2words[indexes[3].item()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
